{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PhyloNext","text":"<p>PhyloNext is an automated pipeline for the analysis of phylogenetic diversity using GBIF occurrence data, species phylogenies from Open Tree of Life, and Biodiverse software.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>PhyloNext pipeline brings together two critical research data infrastructures, the Global Biodiversity Information Facility (GBIF) and Open Tree of Life (OToL), to make them more accessible to non-experts.</p> <p>The pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.</p> <p>The pipeline could be launched in a cloud environment (e.g., the Microsoft Azure Cloud Computing Services, Amazon AWS Web Services, and Google Cloud Computing Services).</p>"},{"location":"#pipeline-summary","title":"Pipeline summary","text":"<ol> <li>Filtering of GBIF species occurrences for various taxonomic clades and geographic areas</li> <li>Removal of non-terrestrial records and spatial outliers (using density-based clustering)</li> <li>Preparation of phylogenetic tree (using pre-constructed phylogenetic tree provided by user or phylogenetic tree can be downloaded automatically using API) and name-matching with GBIF species keys</li> <li>Spatial binning of species occurrences using Uber\u2019s H3 system (hexagonal hierarchical spatial index)</li> <li>Estimation of phylogenetic diversity and endemism indices using Biodiverse program</li> <li>Visualization of the obtained results</li> </ol>"},{"location":"#phylonext-workflow","title":"PhyloNext Workflow","text":""},{"location":"#documentation-overview","title":"Documentation Overview","text":"<p>Installation More detailed information for getting PhyloNext set up on your system.</p> <p>Usage instructions A subset of parameters users may commonly adjust.</p> <p>Input data Description of the input data used by pipeline.</p> <p>Output Overview Overview of PhyloNext output.</p> <p>Pipeline parameters The full set of parameters that users can tweak in PhyloNext.</p> <p>Diversity indices A list of the most common diversity indices.</p> <p>Troubleshooting Handling errors.</p> <p>Web GUI Web-based graphical user interface for PhyloNext.</p> <p>Post processing Post processing output files.</p> <p>Use case examples A few use cases analysed by PhyloNext.  </p> <p>Acknowledgments Acknowledgments and a list of software packages used by PhyloNext.  </p>"},{"location":"#contributions-and-support","title":"Contributions and Support","text":"<p>If your ran into any issues, please let us know by submitting a GitHub Issue.</p> <p>If you would like to contribute to this pipeline, please see the contributing guidelines.</p> <p>PhyloNext source code is deposited here.</p>"},{"location":"acknowledgements/","title":"Acknowledgments","text":"<p>Nearly every component PhyloNext uses, from the workflow, datasets, software packages, even the framework for this site, was created by others and made freely available to the public. We are very grateful to the authors of these software packages and public datasets!</p> <p>Please Cite Datasets and Tools</p> <p>If you have used PhyloNext in your work, please be sure to cite any datasets or software you may have used. For details, see Citations section.</p>"},{"location":"acknowledgements/#funding","title":"Funding","text":"<p>The work is supported by a grant \u201cPD (Phylogenetic Diversity) in the Cloud\u201d to GBIF Supplemental funds from the GEO-Microsoft Planetary Computer Programme.</p>"},{"location":"biodiverse/","title":"Diversity indices","text":"<p>The Biodiverse program is able to estimate more than 350 diversity indices. Here you will find a short list of the most common indices and a way to specify them in PhyloNext.  </p> <p>Indices supported by Biodiverse</p> <p>For details see the Biodiverse manual here: https://github.com/shawnlaffan/biodiverse/wiki/Indices </p> <p>In Biodiverse, indices are organized into groups by \"subroutines\" (functions) used to generate them.  The program's output will include single or multiple diversity metrics depending on the subroutine.  E.g., subroutine <code>calc_simpson_shannon</code> will estimate 4 diversity indices (<code>SHANNON_E</code>, <code>SHANNON_H</code>, <code>SHANNON_HMAX</code>, and <code>SIMPSON_D</code>),  while subroutine <code>calc_phylo_aed_t</code> will output only a <code>PHYLO_AED_T</code> index.  </p> <p>To specify multiple diversity metrics in PhyloNext, a user must provide a comma-separated list of Biodiverse's subroutines to the <code>--indices</code> parameter.  For example, <code>--indices \"calc_pd,calc_pe\"</code>.  The output will contain all metrics estimated by these subroutines (<code>PD</code>, <code>PD_P</code>, <code>PD_P_per_taxon</code>, <code>PD_per_taxon</code>, <code>PE_WE</code>, and <code>PE_WE_P</code> in this case).  </p> Index name 1 Description Subroutine 2 PD Faith's Phylogenetic diversity <code>calc_pd</code> PD_P Phylogenetic diversity as a proportion of total tree length <code>calc_pd</code> PD_per_taxon Phylogenetic diversity per taxon <code>calc_pd</code> PD_P_per_taxon Phylogenetic diversity per taxon as a proportion of total tree length <code>calc_pd</code> ENDW_WE Weighted endemism <code>calc_endemism_whole</code> ENDW_CWE Corrected weighted endemism <code>calc_endemism_whole</code> PE_WE Phylogenetic weighted endemism <code>calc_pe</code> PE_WE_P Phylogenetic weighted endemism as a proportion of the total tree length <code>calc_pe</code> PE_CWE Corrected weighted endemism. This is the phylogenetic analogue of corrected weighted endemism <code>calc_phylo_corrected_weighted_endemism</code> PD_ENDEMISM Phylogenetic Diversity Endemism <code>calc_pd_endemism</code> PD_ENDEMISM_P Phylogenetic Diversity Endemism, as a proportion of the whole tree <code>calc_pd_endemism</code> PHYLO_RPD1 Relative Phylogenetic Diversity (type 1) <code>calc_phylo_rpd1</code> PHYLO_RPD2 Relative Phylogenetic Diversity (type 2) <code>calc_phylo_rpd2</code> PHYLO_RPE1 Relative Phylogenetic Endemism (type 1) <code>calc_phylo_rpe1</code> PHYLO_RPE2 Relative Phylogenetic Endemism (type 2) <code>calc_phylo_rpe2</code> PMPD1_MEAN Mean pairwise phylogenetic distance (MPD) <code>calc_phylo_mpd_mntd1</code> PNTD1_MEAN Mean nearest taxon distance (MNTD) <code>calc_phylo_mpd_mntd1</code> RICHNESS_ALL Species richness <code>calc_richness</code> SIMPSON_D Simpson's diversity index <code>calc_simpson_shannon</code> SHANNON_H Shannon's diversity index <code>calc_simpson_shannon</code> SHANNON_E Shannon's evenness <code>calc_simpson_shannon</code> SHANNON_HMAX Maximum possible value of Shannon's H <code>calc_simpson_shannon</code> ES_x * Hurlbert's ES index (a measure of species diversity that accounts for sample size) <code>calc_hurlbert_es</code> <p>1:      To visualize result on a map, provide the index name to the <code>--leaflet_var</code> parameter.</p> <p>2:      Subroutines are specified with <code>--indices</code> parameter.      Each subroutine should be specified only once.</p> <p>* Hurlbert's ES index</p> <p>To activate the corresponding subroutine in Biodiverse and estimate the ES index,  add <code>calc_hurlbert_es</code> to the <code>--indices</code> parameter (e.g., <code>--indices calc_richness,calc_pd,calc_hurlbert_es</code>). It's important to note that the current implementation of Biodiverse uses a fixed set of ES values:  5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000.  The estimation of a specific index will largely depend on data availability from GBIF,  so it can be challenging to anticipate whether a particular index can be estimated for a given taxon and area of interest.  </p> <p>In PhyloNext, we ensure that all available ES results are exported in a tabular format. If the user is looking to visualize this data on a map, they need to specify the index in the form  <code>ES_x</code>, where <code>x</code> corresponds to one of the ES values supported by Biodiverse (e.g., <code>--leaflet_var RICHNESS_ALL,PD,ES_50,ES_100</code>).</p> <p>In its original formulation, the Hurlbert's  ES index requires the number of individuals per species. However, given the nature of GBIF data, we usually only have information on species occurrences. To overcome this limitation, we can replace the number of individuals with the number of records in the database. This substitution would slightly modify the interpretation of the index,  and the ES index would correspond to the number of unique species in a random sample of N occurrence records. This is an approach that has been previously explored by John Waller  (link to blogpost).  </p> <p>For more information about the ES index, see: Hurlbert SH (1971) The Nonconcept of Species Diversity: A Critique and Alternative Parameters. Ecology, 52: 577-586.  DOI:10.2307/1934145 </p>"},{"location":"biodiverse/#effect-sizes","title":"Effect sizes","text":"<p>Standardized effect sizes (SES; a.k.a. Z-scores) can be used to compare phylogenetic diversity of different communities.  In general, SES is a measure of the deviation of the observed community structure from that expected under a null model of random species distribution.  The null model is obtained by estimating the diversity index of interest by simulating multiple random communities (of the same size). SES can be computed for different diversity metrics.  The most commonly known metrics are NRI (net relatedness index = SES for MPD) and NTI (nearest taxon index = SES for MNTD).  </p> <p>By default, PhyloNext estimates SES for most of the supported indices.  To visualize these metrics, user may add <code>SES_</code> prefix to the index name (e.g., <code>--leaflet_var SES_PD</code>).</p> <p>Standardized effect sizes</p> <p>For more information about standardized effect sizes, see: Webb CO, Ackerly DD, McPeek MA, Donoghue MJ (2002). Phylogenies and Community Ecology. Annual Review of Ecology and Systematics, 33(1), 475\u2013505. DOI:10.1146/annurev.ecolsys.33.010802.150448</p>"},{"location":"biodiverse/#special-indices","title":"Special indices","text":"<p>Several metrics are calculated internally by PhyloNext.  To show them on a map, user may add them to the <code>--leaflet_var</code> parameter.  </p> Index name Description CANAPE Categorical Analysis of Neo and Palaeo Endemism Redundancy Measure of sampling completeness (ratio of species richness and the number of records) <p>CANAPE</p> <p>For more information about the CANAPE method, see: Mishler BD, Knerr N, Gonz\u00e1lez-Orozco CE, Thornhill AH, Laffan CW, Miller\u00a0JT (2014) Phylogenetic measures of biodiversity and neo- and paleo-endemism in Australian Acacia. Nat Commun 5, 4473. DOI:10.1038/ncomms5473</p> <p>Redundancy</p> <p>For more information about the redundancy index, see: Mishler BD, Guralnick R, Soltis PS, Smith SA, Soltis DE, Barve N, Allen JM, Laffan SW (2020) Spatial phylogenetics of the North American flora. J. Syst. Evol., 58: 393-405. DOI:10.1111/jse.12590](https://onlinelibrary.wiley.com/doi/full/10.1111/jse.12590)</p>"},{"location":"citations/","title":"Citations","text":"<p>If you use PhyloNext pipeline for your analysis, please cite it using the following DOI: 10.5281/zenodo.7974081</p> <p>Biodiverse: Laffan SW, Lubarsky E, Rosauer DF (2010) Biodiverse, a tool for the spatial analysis of biological and related diversity. Ecography, 33: 643-647. DOI: 10.1111/j.1600-0587.2010.06237.x </p> <p>CANAPE analysis: Mishler BD, Knerr N, Gonz\u00e1lez-Orozco CE, Thornhill AH, Laffan SW, Miller JT (2014) Phylogenetic measures of biodiversity and neo- and paleo-endemism in Australian Acacia. Nature Communications, 5(1), 4473. DOI:10.1038/ncomms5473 </p> <p>Open Tree of Life synthetic phylogeny: Hinchliff CE, Smith SA, Allman JF, Burleigh JG, Chaudhary R, Coghill LM, Crandall KA, Deng J, Drew BT, Gazis R, Gude K, Hibbett DS, Katz LA, Laughinghouse HD, McTavish EJ, Midford PE, Owen CL, Ree RH, Rees JA, Cranston KA (2015) Synthesis of phylogeny and taxonomy into a comprehensive tree of life. Proceedings of the National Academy of Sciences, 112(41), 12764\u201312769. DOI:10.1073/pnas.1423041112 </p>"},{"location":"citations/#pipeline-dependencies","title":"Pipeline dependencies","text":"<p>Nextflow: Di Tommaso P, Chatzou M, Floden EW, Barja PP, Palumbo E, Notredame C (2017) <code>Nextflow</code> enables reproducible computational workflows. Nature Biotechnology, 35(4), 316\u2013319. DOI:10.1038/nbt.3820 </p> <p>Apache arrow: Richardson N, Cook I, Crane N, Dunnington D, Fran\u00e7ois R, Keane J, Moldovan-Gr\u00fcnfeld D, Ooms J (2022) <code>arrow</code>: Integration to Apache Arrow.  https://arrow.apache.org/docs/r/ </p> <p>data.table: Dowle M, Srinivasan A (2022) <code>data.table</code>: Extension of <code>data.frame</code>. https://r-datatable.com </p> <p>sf: Pebesma E (2018) Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439.  DOI:10.32614/RJ-2018-009 </p> <p>H3 bindings: Kuethe S (2021) <code>h3</code>: R Bindings for H3. https://github.com/crazycapivara/h3-r </p> <p>rgbif: Chamberlain SA, Boettiger C (2017) R, Python, and Ruby clients for GBIF species occurrence data. PeerJ Preprints, 5, e3304v1.  DOI:10.7287/peerj.preprints.3304v1 </p> <p>rotl: Michonneau F, Brown JW, Winter DJ (2016) <code>rotl</code>: An R package to interact with the Open Tree of Life data. Methods in Ecology and Evolution, 7(12), 1476\u20131481. DOI:10.1111/2041-210X.12593 </p> <p>leaflet: Cheng J, Karambelkar B, Xie Y (2022) <code>leaflet</code>: Create interactive web maps with the JavaScript \u201cLeaflet\u201d library.  https://rstudio.github.io/leaflet/</p>"},{"location":"citations/#datasets","title":"Datasets","text":"<p>World base map and polygons of land and urban areas: Natural Earth </p> <p>WGSRPD (World Geographic Scheme for Recording Plant Distributions): Brummitt RK, Pando F, Hollis S, Brummitt NA (2001) World Geographic Scheme for Recording Plant Distributions. http://rs.tdwg.org/wgsrpd/doc/data/</p> <p>Bio-geographical regions for terrestrial animals: Shen Q, Lu J, Zhang S, You Z, Ren Y, Shen X (2022) The Bio-Geographical Regions Division of Global Terrestrial Animal by Multivariate Similarity Clustering Analysis Method. Open Journal of Ecology, 12(3), Article 3. DOI:10.4236/oje.2022.123014</p> <p>CoordinateCleaner-based datasets for removal of common geospatial errors: Zizka A, Silvestro D, Andermann T, Azevedo J, Duarte Ritter C, Edler D, Farooq H, Herdean A, Ariza M, Scharn R, Svantesson S, Wengstr\u00f6m N, Zizka V, Antonelli A (2019) <code>CoordinateCleaner</code>: Standardized cleaning of occurrence records from biological collection databases. Methods in Ecology and Evolution, 10(5), 744\u2013751. DOI:10.1111/2041-210X.13152 </p>"},{"location":"inputdata/","title":"Obtaining the input data for PhyloNext","text":""},{"location":"inputdata/#gbif-mediated-data-in-parquet-format","title":"GBIF-mediated data in Parquet format","text":"<p>GBIF provides monthly-based periodic snapshots of species occurrence records.  These snapshots are available on Amazon S3, Google GCS, and Microsoft Azure cloud storages.  Data are stored in Apache Parquet format files.  Parquet format allows running queries quickly and efficiently.</p> <p>Data format</p> <p>For more information on the data format, see the description of GBIF Public Datasets.</p>"},{"location":"inputdata/#obtaining-a-local-snapshot-of-species-occurrences-from-gbif","title":"Obtaining a local snapshot of species occurrences from GBIF","text":"<p>If you would like to run the pipeline locally, you may download the full GBIF occurrence dump from the Amazon AWS cloud using the AWS CLI program (no AWS account required). E.g., to download <code>2022-05-01</code> dump to the <code>GBIF</code> directory in your home folder run: <pre><code>aws s3 sync \\\ns3://gbif-open-data-eu-central-1/occurrence/2022-05-01/occurrence.parquet/ \\\n~/GBIF/ \\\n--no-sign-request\n</code></pre></p> <p>Alternative GBIF snapshot is also hosted by the Microsoft AI for Earth program. To download it using the AzCopy command-line utility run: <pre><code>azcopy copy \\\n\"https://ai4edataeuwest.blob.core.windows.net/gbif/occurrence/2022-05-01/occurrence.parquet/*\" \\\n\"~/GBIF/\" \\\n--recursive=true\n</code></pre></p> <p>A list of snapshots with download links is available at https://www.gbif.org/occurrence-snapshots.</p> <p>Limitations</p> <p>Currently, <code>arrow</code> v.10.0.0 does not support filtering of array-type columns. E.g., it could be useful for removal of common geospatial issues in the data (see column <code>issue</code> in the parquet files).</p> <p>For developers</p> <p>See the related issues ARROW-16641  and ARROW-16702  at Apache Arrow issue tracker.</p> <p>If you would like to remove records with geospatial issues from the data, please see the following section.</p>"},{"location":"inputdata/#subsetting-occurrence-data-via-gbif-api","title":"Subsetting occurrence data via GBIF API","text":"<p>Working with a subset of occurrences could increase the processing speed of the pipeline.  It is possible to obtain a subset of species occurrence from GBIF programmatically.  For this purpose, one may use an application programming interface (API) provided by GBIF.  API is a set of rules and protocols that allow different software programs to communicate with each other.  Therefore, APIs enable developers to access the data and functionality of a website or web-based service in a controlled, programmatic way.  </p> <p>First, you must specify a set of filters that should be applied to the data.  For this purpose, create a simple text file in JSON format (see the example below).  Please replace <code>creator</code> and <code>notification_address</code> with your own.  </p> <pre><code>{\n\"format\": \"SIMPLE_PARQUET\",\n\"creator\": \"USERNAME\",\n\"notification_address\": [ \"USEREMAIL\" ],\n\"predicate\": {\n\"type\": \"and\",\n\"predicates\": [\n{\n\"type\": \"equals\",\n\"key\": \"OCCURRENCE_STATUS\",\n\"value\": \"PRESENT\"\n},\n{\n\"type\": \"equals\",\n\"key\": \"HAS_GEOSPATIAL_ISSUE\",\n\"value\": \"false\"\n},\n{\n\"type\": \"not\",\n\"predicate\": {\n\"type\": \"in\",\n\"key\": \"ESTABLISHMENT_MEANS\",\n\"values\": [\n\"MANAGED\",\n\"INTRODUCED\",\n\"INVASIVE\",\n\"NATURALISED\"\n]\n}\n},\n{\n\"type\": \"not\",\n\"predicate\": {\n\"type\": \"in\",\n\"key\": \"BASIS_OF_RECORD\",\n\"values\": [\n\"FOSSIL_SPECIMEN\",\n\"LIVING_SPECIMEN\"\n]\n}\n},\n{\n\"type\": \"not\",\n\"predicate\": {\n\"type\": \"in\",\n\"key\": \"ISSUE\",\n\"values\": [\n\"TAXON_MATCH_HIGHERRANK\"\n]\n}\n}\n]\n}\n}\n</code></pre> <p>Note the usage of <code>HAS_GEOSPATIAL_ISSUE</code>, which is a shortcut for the following issues:  <code>ZERO_COORDINATE</code>, <code>COORDINATE_INVALID</code>, <code>COORDINATE_OUT_OF_RANGE</code>, and <code>COUNTRY_COORDINATE_MISMATCH</code>.  </p> <p>Geospatial filters &amp; issues</p> <p>To read more about different Geospatial issues, see https://docs.gbif.org/course-data-use/en/geospatial-filters-issues.html</p> <p>If you would like to specify taxonomic or spatial scopes,  you may add additional predicates (query expressions to retrieve occurrence record downloads), e.g.:  </p> <pre><code>   {\n\"type\": \"equals\",\n\"key\": \"GENUS_KEY\",\n\"value\": \"2978223\"\n},\n{\n\"type\": \"equals\",\n\"key\": \"COUNTRY\",\n\"value\": \"AU\"\n}\n</code></pre> <p>GBIF predicates</p> <p>An extensive list of supported predicates and query parameters could be found here: https://www.gbif.org/developer/occurrence</p> <p>To send download request, please fill in your user name and password and run: <pre><code>USER=\"USERNAME\"\nPASSWORD=\"PASSWORD\"\ncurl -Ssi \\\n--user \"$USER\":\"$PASSWORD\" \\\n-H \"Content-Type: application/json\" \\\n-X POST -d @get_parquet.json \\\nhttps://api.gbif.org/v1/occurrence/download/request\n</code></pre></p> <p>Please note the download ID returned by <code>curl</code>. To check the status of the request and get the download link, use: <pre><code>curl -Ss https://api.gbif.org/v1/occurrence/download/0003936-220831081235567 | jq .\n</code></pre> (don't forget to replace the download ID!)</p> <p>To download the results (zip archive with parquet files), use: <pre><code>mkdir -p ~/GBIF_dumps\ncd ~/GBIF_dumps\naria2c \\\nhttps://api.gbif.org/v1/occurrence/download/request/0003936-220831081235567.zip \\\n-o gbif_dump.zip\n</code></pre></p> <p>GBIF API beginners guide</p> <p>A very nice introduction to the GBIF APIs can be found in the \"GBIF API beginners guide\" by John Waller.</p> <p>Required software</p> <p>To run the commands mentioned above, you may need to install the following software: <code>curl</code>, <code>jq</code>, and <code>aria2</code>. If you are using <code>conda</code> package manager, it could be done in a singe command: <code>conda install -c conda-forge curl aria2 jq</code></p>"},{"location":"inputdata/#obtaining-a-list-of-specieskeys-for-extinct-species","title":"Obtaining a list of specieskeys for extinct species","text":"<p>To get a list of extinct species from GBIF, users may run the <code>fetch_gbif_extinct_tax.py</code> script.</p> <p>Download the script: <pre><code>wget https://raw.githubusercontent.com/vmikk/biodiverse-scripts/main/bin/fetch_gbif_extinct_tax.py\nchmod +x fetch_gbif_extinct_tax.py\n</code></pre></p> <p>To use the script, please specify the GBIF taxon ID of the group of interest (e.g., 359 for Mammalia). <pre><code>## Reptilia [358]\n# https://www.gbif.org/species/358\npython fetch_gbif_extinct_tax.py 358 --outfile extinct_tax_ids_Reptilia.txt\n\n## Mammalia [359]\n# https://www.gbif.org/species/359\npython fetch_gbif_extinct_tax.py 359 --outfile extinct_tax_ids_Mammalia.txt\n\n## Amphibia  [131]\n# https://www.gbif.org/species/131\npython fetch_gbif_extinct_tax.py 131 --outfile extinct_tax_ids_Amphibia.txt\n\n## Birds - class Aves [212]\n# https://www.gbif.org/species/212\npython fetch_gbif_extinct_tax.py 212 --outfile extinct_tax_ids_Birds.txt\n\n## Tracheophyta [7707728]\n# https://www.gbif.org/species/7707728\npython fetch_gbif_extinct_tax.py 7707728 --outfile extinct_tax_ids_Tracheophyta.txt\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":"<p>PhyloNext was primarily developed for Linux. To run it from a command line on Windows, please use Windows Subsystem for Linux (WSL).</p> <p>All pipeline dependencies were encapsulated in containers (Docker and Singularity), which will be automatically downloaded with the first run of PhyloNext. Only the main dependencies must be installed manually. They include:  </p> <ul> <li>Java </li> <li>Nextflow </li> <li>Container engine (either Docker or Singularity)  </li> </ul> <p>The following installation instruction is for Ubuntu.</p>"},{"location":"installation/#1-install-nextflow","title":"1. Install <code>Nextflow</code>","text":"<p>Nextflow (<code>&gt;=22.10.0</code>) requires Java 11 (or later, up to 18) to be installed.</p> <pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n</code></pre> <p>Install Nextflow:</p> <pre><code>wget -qO- https://get.nextflow.io | bash\nchmod +x ./nextflow\nmkdir -p ~/bin &amp; mv ./nextflow ~/bin\n</code></pre> <p>Conda</p> <p>Alternatively, one can install Nextflow via conda, a package and environment management system. See here for additional instructions.</p>"},{"location":"installation/#2-install-docker","title":"2. Install <code>Docker</code>","text":"<p>Docker is a tool designed to make it easier to create, deploy, and run applications by using containers.  Containers allow you to package an application with all of the parts it needs, such as libraries and other dependencies,  and ship it all out as one package. This eliminates the need to worry about installing and configuring the necessary dependencies on the host system.</p>"},{"location":"installation/#ubuntu","title":"Ubuntu","text":"<p>For more details see the official Docker documentation.</p> <pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl gnupg lsb-release\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\necho \\\n\"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> <p>Docker via package manager</p> <p>Alternatively, one can install Docker via the system's package manager (e.g. <code>Ubuntu Software Center</code>).</p>"},{"location":"installation/#windows","title":"Windows","text":"<p>It would be easier to use Docker Desktop WSL 2 backend on Windows.  Docker Desktop is an application for Windows that provides an easy-to-use interface for managing Docker containers.  It includes tools for building and deploying containerized applications, as well as for managing the lifecycle of containers.</p> <p>To install Docker on Windows, you will need to follow these steps:  </p> <ul> <li>Download the Docker Desktop installer from the Docker website https://www.docker.com/products/docker-desktop </li> <li>Double-click the installer to begin the installation process  </li> <li>Follow the prompts to complete the installation. This will include accepting the terms of the license agreement and selecting the installation location  </li> <li>Once the installation is complete, you will need to restart your computer to complete the installation process  </li> </ul> <p>For more details see the Docker Desktop documentation.</p>"},{"location":"installation/#3-download-the-pipeline-and-test-it-on-a-minimal-dataset-with-a-single-command","title":"3. Download the pipeline and test it on a minimal dataset with a single command","text":"<pre><code>nextflow run vmikk/phylonext -r main -profile test,docker\n</code></pre>"},{"location":"outputs/","title":"PhyloNext output","text":""},{"location":"outputs/#diversity-estimates-in-tabular-format","title":"Diversity estimates in tabular format","text":"<p>Table <code>03.Plots/Biodiverse_results_merged.txt</code></p>"},{"location":"outputs/#interactive-map-with-diversity-values","title":"Interactive map with diversity values","text":"<p>The leaflet-based map is located in <code>03.Plots/Choropleth.html</code>.  </p> <p>As a small example (based on the Acacia dataset from Australia, H3 resolution = 3), a map will look like this:  </p> <p>You may toggle selectors at the top-right corner to show the results for a particular diversity metric on the map.  In the example, there are three indices available:  </p> <ul> <li><code>RICHNESS_ALL</code>, observed species richness;  </li> <li><code>SES_PD</code>, the effect size for the phylogenetic diversity;  </li> <li><code>CANAPE</code>, centers of neo- and paleo-endemism (according to the CANAPE analysis; see Mishler et al., 2020 for details).  </li> </ul> <p>Use the pipeline's <code>--leaflet var</code> parameter to specify which indices will be displayed on a map. </p>"},{"location":"outputs/#geopackage","title":"GeoPackage","text":"<p>Diversity estimates and H3 polygons are also exported into the GeoPackage file (<code>03.Plots/Diversity_estimates.gpkg</code>).  OGC GeoPackage is a relatively new, open standard data format for storing geospatial data.  It is designed to be an efficient, portable, compact, and self-contained format that can store and exchange data within a single file. </p> <p>GeoPackage is supported in most modern GIS software packages (e.g., QGIS).  See the example of opening GeoPackage in QGIS in the post-processing section.</p> <p>Why not shapefiles?</p> <p>More information on the comparisons of different formats for storing geospatial data can be found here: http://switchfromshapefile.org/</p>"},{"location":"outputs/#dataset-information","title":"Dataset information","text":"<p>The pipeline output contains two files with references to the original data sources,  which can be used for citing the data and giving proper credit to the original data providers  who collected and curated the data.  </p> <p>Species occurrences - <code>results/pipeline_info/Dataset_DOIs.txt</code> Phylogenetic tree - <code>02.OTT_tree/citations.txt</code> </p> <p>The <code>Dataset_DOIs.txt</code> file also contains Digital Object Identifiers (DOIs) for the datasets,  which can be used to create a derived dataset of species occurrences.  For more information, see the <code>Derived datasets</code> paragraph in the post-processing section.</p>"},{"location":"outputs/#pipeline-summary","title":"Pipeline summary","text":"<p>In the <code>results/pipeline_info</code> directory, there are pipeline execution reports created by Nextflow.  These files in HTML format includes the workflow report and execution timeline for each process.  </p> <p>Workflow report contains many useful metrics about pipeline execution.  </p> <p>In particular, the Resources section plots the distribution of resource usage (CPU, memory, job duration, and disk I/O)  for each workflow process using the interactive plotly.js plotting library.  The plots have several tabs with the raw values and a percentage representation showing what proportion of the requested resources were used.  These plots are very helpful in checking that task resources are used efficiently.  </p> <p>The Tasks section lists all executed tasks, reporting the status, the actual command script, and many other metrics for each of the tasks.  The same information is also stored in a tabular format in tab-delimited format (e.g., <code>execution_trace_*.txt</code>).  </p> <p>Execution tracing file contains helpful information about each process, including submission time, start time, completion time, CPU, and memory used.  </p> <p>Resource usage metrics</p> <p>To learn more about how resource usage is computed by Nextflow, see: https://www.nextflow.io/docs/latest/tracing.html#trace-report and https://www.nextflow.io/docs/latest/metrics.html#metrics-page</p>"},{"location":"parameters/","title":"Pipeline parameters","text":"<p>PhyloNext includes numerous configurable parameters. For convenience, they are grouped into several categories: </p> <ul> <li>Input/output options</li> <li>Data subsetting:<ul> <li>Taxonomic scope</li> <li>Spatial scope</li> </ul> </li> <li>Spatial outliers removal</li> <li>Occurrence filtering and binning</li> <li>Diversity estimation</li> <li>Visualization<ul> <li>Interactive (Leaflet-based)</li> <li>Static (maps in pdf format)</li> </ul> </li> <li>Phylogenetic tree-related parameters</li> <li>Generic options</li> <li>Nextflow-specific parameters</li> </ul>"},{"location":"parameters/#inputoutput-options","title":"Input/output options","text":"<p>Define where the pipeline should find input data and save output data.</p> Parameter Description Type Default <code>--input</code> Path to the directory with parquet files (Parquet format) 1 <code>directory</code> <code>--outdir</code> The output directory where the results will be saved <code>directory</code> ./results <code>--phytree</code> Custom phylogenetic tree in Newick format (optional) 2 <code>file</code> <p>1:     GBIF occurrence dump in the Parquet format.      Could be stored locally or in the cloud (S3 or Azure Blob storage).</p> <p>2:     Users have the option to provide their own custom phylogenetic tree.      If this option is chosen, the tips of the phylogenetic tree should be labeled      either with Latin binomials (for example, \"Homo_sapiens\"),      or with Open Tree IDs (for example, \"ott359899\").      Be sure to adjust the <code>--phylabels</code> parameter accordingly.     Additionally, a set of phylogenetic trees, which come with descriptions and      are pre-packaged with the pipeline, are available at this link: https://github.com/vmikk/PhyloNext/tree/main/test_data/phy_trees </p>"},{"location":"parameters/#taxonomic-scope","title":"Taxonomic scope","text":"<p>Define which taxa should be analyzed.</p> Parameter Description Type Example Default <code>--phylum</code> Phylum to analyze 1 <code>string</code> \"Chordata\" <code>--classis</code> Class to analyze 1 2 <code>string</code> \"Mammalia\" <code>--order</code> Order to analyze 1 <code>string</code> \"Carnivora\" <code>--family</code> Family to analyze 1 <code>string</code> \"Felidae,Canidae\" <code>--genus</code> Genus to analyze 1 <code>string</code> \"Felis,Canis,Lynx\" <code>--specieskeys</code> Custom list of GBIF specieskeys (file with a single column) <code>file</code> \"Carnivora\" <code>--noextinct</code> File with extinct species specieskeys for their removal (file with a single column, with header) <code>file</code> \"Carnivora\" <code>--excludehuman</code> Exclude genus \"Homo\" from occurrence data <code>boolean</code> True True <p>1:      Multiple comma-separated values allowed (e.g., \"Felidae,Canidae\" for the Family rank).  </p> <p>2:     Unfortunately, <code>class</code> is a reserved keyword in Nextflow.      Therefore, Latin <code>classis</code> is used as a parameter name.</p>"},{"location":"parameters/#spatial-scope","title":"Spatial scope","text":"<p>Spatial filters.</p> Parameter Description Type Example Default <code>--latmin</code> Minimum latitude of species occurrences 1 <code>number</code> 5.1 <code>--latmax</code> Maximum latitude of species occurrences 1 <code>number</code> 15.5 <code>--lonmin</code> Minimum longitude of species occurrences 1 <code>number</code> 47.0 <code>--lonmax</code> Maximum longitude of species occurrences 1 <code>number</code> 55.5 <code>--country</code> Country code, ISO 3166 format 2 <code>string</code> \"DE,PL,CZ\" <code>--polygon</code> Custom area of interest (a file with polygons in GeoPackage format) <code>file</code> <code>Brazil_ThreeStates.gpkg</code> <code>--wgsrpd</code> Polygons of World Geographical Regions 3 <code>file</code> <code>pipeline_data/WGSRPD.RData</code> <code>--regions</code> Names of World Geographical Regions 4 <code>string</code> \"L1_EUROPE,L1_ASIA_TEMPERATE\" <p>1: Coordinates should be provided in decimal degrees</p> <p>2:     Country codes should be provided in the two-letter <code>ISO 3166-1 alpha-2</code> coding system,     see details here (column <code>Alpha-2</code> code),     and this Wikipedia article.</p> <p>3:     PhyloNext ships WGSRPD shapefile as a built-in data.     To use it, specify a full path to the data (e.g., <code>$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/WGSRPD.RData\")</code>)</p> <p>4:     Multiple comma-separated values allowed.  </p> <p>Currently, Level-1 and Level-2 WGSRPD polygons are supported.  </p> <p>Level-1 polygons (continental level):  </p> <p>Level-2 polygons (regional or subcontinental level):  </p> Number Polygon code 1 <code>L1_EUROPE</code> 2 <code>L1_AFRICA</code> 3 <code>L1_ASIA_TEMPERATE</code> 4 <code>L1_ASIA_TROPICAL</code> 5 <code>L1_AUSTRALASIA</code> 6 <code>L1_PACIFIC</code> 7 <code>L1_NORTHERN_AMERICA</code> 8 <code>L1_SOUTHERN_AMERICA</code> 9 <code>L1_ANTARCTIC</code> ------ --------------------------------- 10 <code>L2_Northern_Europe</code> 11 <code>L2_Middle_Europe</code> 12 <code>L2_Southwestern_Europe</code> 13 <code>L2_Southeastern_Europe</code> 14 <code>L2_Eastern_Europe</code> 20 <code>L2_Northern_Africa</code> 21 <code>L2_Macaronesia</code> 22 <code>L2_West_Tropical_Africa</code> 23 <code>L2_West_Central_Tropical_Africa</code> 24 <code>L2_Northeast_Tropical_Africa</code> 25 <code>L2_East_Tropical_Africa</code> 26 <code>L2_South_Tropical_Africa</code> 27 <code>L2_Southern_Africa</code> 28 <code>L2_Middle_Atlantic_Ocean</code> 29 <code>L2_Western_Indian_Ocean</code> 30 <code>L2_Siberia</code> 31 <code>L2_Russian_Far_East</code> 32 <code>L2_Middle_Asia</code> 33 <code>L2_Caucasus</code> 34 <code>L2_Western_Asia</code> 35 <code>L2_Arabian_Peninsula</code> 36 <code>L2_China</code> 37 <code>L2_Mongolia</code> 38 <code>L2_Eastern_Asia</code> 40 <code>L2_Indian_Subcontinent</code> 41 <code>L2_Indo_China</code> 42 <code>L2_Malesia</code> 43 <code>L2_Papuasia</code> 50 <code>L2_Australia</code> 51 <code>L2_New_Zealand</code> 60 <code>L2_Southwestern_Pacific</code> 61 <code>L2_South_Central_Pacific</code> 62 <code>L2_Northwestern_Pacific</code> 63 <code>L2_North_Central_Pacific</code> 70 <code>L2_Subarctic_America</code> 71 <code>L2_Western_Canada</code> 72 <code>L2_Eastern_Canada</code> 73 <code>L2_Northwestern_USA</code> 74 <code>L2_North_Central_USA</code> 75 <code>L2_Northeastern_USA</code> 76 <code>L2_Southwestern_USA</code> 77 <code>L2_South_Central_USA</code> 78 <code>L2_Southeastern_USA</code> 79 <code>L2_Mexico</code> 80 <code>L2_Central_America</code> 81 <code>L2_Caribbean</code> 82 <code>L2_Northern_South_America</code> 83 <code>L2_Western_South_America</code> 84 <code>L2_Brazil</code> 85 <code>L2_Southern_South_America</code> 90 <code>L2_Subantarctic_Islands</code> 91 <code>L2_Antarctic_Continent</code> <p>Custom polygons</p> <p>It is possible to use custom polygons instead of WGSRPD file.  Polygons should be in the simple feature collection format (class <code>sf</code> of the <code>sf</code> package,  geometry type: <code>MULTIPOLYGON</code>) and must contain <code>LevelName</code> column. Data should be saved as a serialized R object (with <code>saveRDS</code>).</p>"},{"location":"parameters/#spatial-outliers-removal","title":"Spatial outliers removal","text":"Parameter Description Type Example Default <code>--dbscan</code> Remove spatial outliers with density-based clustering <code>boolean</code> True False <code>--dbscannoccurrences</code> Minimum species occurrence to perform DBSCAN <code>integer</code> 30 30 <code>--dbscanepsilon</code> DBSCAN parameter epsilon, km <code>integer</code> 1500 1500 <code>--dbscanminpts</code> DBSCAN min number of points <code>integer</code> 3 3 <p>Currently, only density-based clustering (DBSCAN) algorithm is implemented for removal of spatial outliers. The DBSCAN algorithm requires 2 parameters: - <code>eps</code> (<code>--dbscanepsilon</code>), which specifies how close points should be to each other to be considered a part of a cluster   If the distance between two points is lower or equal to this value, these points are considered neighbors. - <code>minPoints</code> (<code>--dbscanminpts</code>), the minimum number of points to form a dense region.  </p> <p>Parameter <code>--dbscannoccurrences</code> is used to skip DBSCAN filtering for species with low number of unique points (e.g., &lt;30).  </p> <p>For more details, see the blog post \"Outlier Detection Using DBSCAN\" by John Waller.</p>"},{"location":"parameters/#occurrence-filtering-and-binning","title":"Occurrence filtering and binning","text":"Parameter Description Type Example Default <code>--minyear</code> Minimum year of record's occurrences <code>integer</code> 2000 1945 <code>--maxyear</code> Maximum year of record's occurrences <code>integer</code> 2010 <code>--coordprecision</code> Coordinate precision threshold, decimal degrees 1 <code>number</code> 0.1 0.1 <code>--coorduncertainty</code> Maximum allowed coordinate uncertainty, meters 1 <code>number</code> 10000 10000 <code>--coorduncertaintyexclude</code> Black list of coordinate uncertainty values 1 <code>string</code> \"9999\" \"301,3036,999,9999\" <code>--basisofrecordinclude</code> Round spatial coordinates to N decimal places 2 <code>string</code> \"PRESERVED_SPECIMEN\" <code>--basisofrecordexclude</code> Round spatial coordinates to N decimal places 2 <code>string</code> \"FOSSIL_SPECIMEN\" \"FOSSIL_SPECIMEN,LIVING_SPECIMEN\" <code>--h3resolution</code> Spatial resolution of the H3 geospatial indexing system <code>integer</code> 4 4 <code>--roundcoords</code> Round spatial coordinates to N decimal places 3 <code>integer</code> 2 2 <p>For spatial binning of species occurrences, PhyloNext uses H3 geospatial indexing system developed by Uber.  H3 represents a hierarchical geospatial index, where each hexagonal grid cell has a unique index (e.g., <code>8a1f05835a37fff</code> for GBIF headquarter).  H3 supports 16 resolutions (from 1 to 16), which can be selected in PhyloNext using <code>--h3resolution</code> parameter.  By default, PhyloNext uses resolution <code>4</code>, which corresponds to a hexagon with edge length of 22.6 km and cell area of 1170 km2.  More details on H3 resolutions could be found here.  </p> <p>1:     It's possible to remove occurrence records with a high level of coordinate uncertainty or low precision.      It can be done using some threshold values.      Also, there are several known default values for coordinate uncertainty in meters, which could be black-listed as well      (e.g., these values may correspond to records linked to country centroids).     NB! Records with missing values will not be removed, as many publishers do not fill these fields in the database.     For details, see the blog post. \"Common things to look out for when post-processing GBIF downloads\" by John Waller.</p> <p>2:     For details, see description of a Darwin Core term Basis of record      and a short explanation here.     Multiple comma-separated values allowed.  </p> <p>3:      As DBSCAN filtering is very computationally intensive,      it is possible to reduce dataset size (almost without loosing precision) by rounding record coordinates.      By default, coordinates are rounded to 2 decimal places,      which corresponds to an accuracy of ~1.11 km at the equator.      To disable coordinate rounding, set <code>--roundcoords</code> to a negative values (e.g., <code>-1</code>).</p>"},{"location":"parameters/#removal-of-common-spatial-errors","title":"Removal of common spatial errors","text":"<p>PhyloNext implements several filters analogous to the filters  in <code>CoordinateCleaner</code> R package by Alexander Zizka:</p> <ul> <li>Country and province centroids</li> <li>Capital coordinates</li> <li>Coordinates of biodiversity institutions</li> <li>Urban areas</li> <li>Seas</li> </ul> Parameter Description Type Default <code>--terrestrial</code> Land polygon for removal of non-terrestrial occurrences <code>file</code> enabled <code>--rmcountrycentroids</code> Polygons with country and province centroids <code>file</code> disabled <code>--rmcountrycapitals</code> Polygons with country capitals <code>file</code> disabled <code>--rminstitutions</code> Polygons with biological institutions and museums <code>file</code> disabled <code>--rmurban</code> Polygons with urban areas <code>file</code> disabled <p>PhyloNext provides files for removal of common spatial errors,   the data are built-in in the <code>pipeline_data</code> direcory and can be selected in the following way: <pre><code>--terrestrial `$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/Land_Buffered_025_dgr.RData\")`\n--rmcountrycentroids `$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/CC_CountryCentroids_buf_1000m.RData\")`\n--rmcountrycapitals `$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/CC_Capitals_buf_10000m.RData\")`\n--rminstitutions `$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/CC_Institutions_buf_100m.RData\")`\n--rmurban `$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/CC_Urban.RData\")`\n</code></pre> Alternatively, users can provide own files with custom polygons.</p>"},{"location":"parameters/#diversity-estimation","title":"Diversity estimation","text":"<p>Diversity estimation is performed using Biodiverse program by Shawn Laffan.  Therefore, parameter values should correspond to the Biodiverse values.  </p> Parameter Description Type Example Default <code>--indices</code> Diversity and endemism indices to estimate 1 <code>string</code> \"calc_richness,calc_pd,calc_pe\" \"calc_richness,calc_simpson_shannon,calc_endemism_whole,calc_pd,calc_pe,calc_phylo_corrected_weighted_endemism,calc_phylo_rpd1,calc_phylo_rpd2,calc_phylo_rpe1,calc_phylo_rpe2\" <code>--iterations</code> Number of randomisation iterations for standardized effect size estimation <code>integer</code> 1000 1000 <code>--biodiversethreads</code> Number of Biodiverse threads <code>integer</code> 10 10 <code>--randname</code> Randomisation scheme type 2 <code>string</code> \"rand_structured\" \"rand_structured\" <code>--randconstrain</code> Polygons to perform spatially constrained randomization (GeoPackage format) 2 <code>file</code> <code>ZoogeographicRegions.gpkg</code> <p>1:      Comma-separated list of metrics. More than 350 indices are supported.     For details see Biodiverse manual.     The most common indices are listed here: <code>Diversity indices</code>.  </p> <p>2:     In order to estimate standardized effect sizes (SES) of different diversity metrics and      determine whether the obtained estimates are more extreme than what might be predicted given a null model,      Biodiverse requires to perform randomization.      By default, Biodiverse uses the <code>rand_structured</code> randomization algorithm,      which shuffles species occurrences randomly across the entire area while preserving the      species richness of each sample or grid cell. This method works well for most studies and is very effective.      However, in cases where the analysis is conducted on a large scale and involves samples from different biomes or continents,      the total pool of species may span multiple environments. As a result, randomizations may allocate      a polar taxon to the tropics, or a desert taxon to a rainforest. One way to address this issue is      by performing randomizations within a subset of data based on a spatial condition, such as biomes or zoogeographic regions.      PhyloNext allows the user to specify such conditions as a GeoPackage file with multiple polygons.      This feature enables users to keep any species within the biome in which they are found while still randomly relocating them.      Hence, the randomization process can be tailored to suit the specific needs of the study      and ensure that the results accurately reflect the distribution of species in the area of interest.</p> <p>Randomization algorithms in Biodiverse</p> <p>See a series of blog posts by Shawn Laffan: 1. Better control of randomisations 2. How the <code>rand_structured</code> algorithm works 3. Spatially partition your randomisations </p> <p>To illustrate how spatially-constrained randomizations can be executed for Mammals on a global scale,  one can utilize the zoogeographical regions established by Shen et al., 2022, DOI:10.4236/oje.2022.123014. These regions are conveniently available in the <code>pipeline_data/ZoogeographicRegions.gpkg</code> file,  which is included in PhyloNext. The file comprises seven biogeographical kingdoms: - West Palaearctic kingdom - East Palaearctic kingdom - Indo-Pacific kingdom - Afrotropical kingdom - Australian kingdom - Nearctic kingdom - Neotropical kingdom  </p> <p> (original data source - https://www.scirp.org/journal/paperinformation.aspx?paperid=116248)</p> <p>To activate spatially-constrained randomizations using these biogeographic regions, add the following argument to your command: <pre><code>--randconstrain `$(realpath \"${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/ZoogeographicRegions.gpkg\")`\n</code></pre></p>"},{"location":"parameters/#visualization-interactive","title":"Visualization - interactive","text":"<p>Interactive visualization depends on Leaflet library.</p> Parameter Description Type Example Default <code>--leaflet_var</code> Diversity and endemism indices to estimate 1 <code>string</code> \"PD,SES_PD\" \"RICHNESS_ALL,PD,SES_PD,PD_P,ENDW_WE,SES_ENDW_WE,PE_WE,SES_PE_WE,PE_CWE,SES_PE_CWE,CANAPE,Redundancy\" <code>--leaflet_color</code> Color scheme for continuous variables  2 <code>string</code> \"RdYlBu\" \"RdYlBu\" <code>--leaflet_palette</code> Color palette for continuous variables 3 <code>string</code> \"quantile\" \"quantile\" <code>--leaflet_bins</code> Number of color bins for continuous variables <code>integer</code> 5 5 <code>--leaflet_sescolor</code> Color scheme for standardized effect sizes, SES 4 <code>string</code> \"hotspots\" \"threat\" <code>--leaflet_redundancy</code> Redundancy threshold for hiding the grid cells with low number of records 5 <code>number</code> 85 0 <code>--leaflet_canapesuper</code> Include the <code>super-endemism</code> class in CANAPE results 6 <code>boolean</code> True False <p>1:     For a list of indices available in Biodiverse, see here.     Note: to show the index on a map, please include appropriate name of the Biodiverse subroutine with <code>--indices</code> parameter.     To display Standardized-Effect-Size-based variables, add <code>SES_</code> prefix to the index name (e.g., <code>SES_PD</code>).     For CANAPE (categorical analysis of neo- and paleoendemism; Mishler et al., 2014), add <code>CANAPE</code> to the variables list.     To display redundancy index (measure of sampling completeness; see Mishler et al., 2020), add <code>Redundancy</code> to the variables list.  </p> <p>2:      The name of a a color scheme from the <code>ColorBrewer</code> designed by Cynthia Brewer.      For more options see here.</p> <p>3:    Defines mapping of data values to colors . See Leaflet docs:  </p> <ul> <li><code>colorNumeric</code> is a simple linear mapping from continuous numeric data to an interpolated palette</li> <li><code>colorBin</code> also maps continuous numeric data, but performs binning based on value (see the cut function)</li> <li><code>colorQuantile</code> similarly bins numeric data, but via the quantile function</li> </ul> <p>4:     The default color scheme (<code>threat</code>-type) for SES values follows Mishler et al., 2014,      where high SES values are in blue, and low SES values are in red.     Alternatively, it is possible to use the <code>hotspot</code>-type palette,      where areas with the red values indicate grid cells that contain significantly higher diversity or endemism than expected (hotspots),      while the blue values indicate grid cells that contain significantly lower diversity/endemism than expected (coldspots).  </p> <p>5:     Sampling redundancy is defined as [1 \u2013 (richness / number of specimens)] in a grid cell.      By default, parameter <code>--leaflet_redundancy</code> is set to 0, which will display all grid cells.</p> <p>6:     In contrast with the original publication by Mishler et al. (2020),      the current version of Biodivere (v.4.1) classifies endemism into 3 categories      (super-endemism type is merged with mixed endemism,      see https://biodiverse-analysis-software.blogspot.com/2022/10/biodiverse-now-calculates-canape-for-you.html).      In PhyloNext, it is possible to use both coloring schemes (with and without super-endemism).  </p> <p>CANAPE</p> <p>CANAPE (Categorical Analysis of Palaeo and Neo Endemism)  is able to distinguish different types of centres of endemism, and can thus give insights  into different evolutionary and ecological processes that may be responsible for these patterns. </p> <ul> <li>The centres of paleo-endemism indicate places where there are over-representation of long branches that are rare across the landscape.</li> <li>The centres of neo-endemism indicate an area where there is an over-representation of short branches that are rare on the landscape.</li> <li>Mixture of both paleo-endemism and neo-endemism</li> <li>Super-endemic sites</li> </ul>"},{"location":"parameters/#visualization-static","title":"Visualization - static","text":"Parameter Description Type Example Default <code>--plotvar</code> Variables to plot 1 <code>string</code> \"PD\" \"RICHNESS_ALL,PD,PD_P\" <code>--plottype</code> Plot type <code>string</code> raw <code>--plotformat</code> Plot format (jpg,pdf,png) <code>string</code> pdf <code>--plotwidth</code> Plot width (default, 18 inches) <code>number</code> 18 <code>--plotheight</code> Plot height (default, 18 inches) <code>number</code> 18 <code>--plotunits</code> Plot size units (in,cm) <code>string</code> in <code>--world</code> World basemap 2 <code>file</code> enabled <p>1: Multiple comma-separated values allowed. For details see <code>--leaflet_var</code> parameter notes.</p> <p>2:     World base map is based on Natural Earth data      and is located in the <code>pipeline_data/WorldMap_NaturalEarth_Medium.RData</code> file.</p>"},{"location":"parameters/#phylogenetic-tree-related-parameters","title":"Phylogenetic tree-related parameters","text":"Parameter Description Type Default <code>--phylabels</code> Type of tip labels on a phylogenetic tree (\"OTT\" or \"Latin\") 1 <code>string</code> Latin <code>--taxgroup</code> Specific taxonomy group in Open Tree of Life (default, \"All_life\") 2 <code>string</code> All_life <code>--maxage</code> Manually assign root age for a tree obtained from Open Tree of Life 3 <code>number</code> <code>--phyloonly</code> Prune Open Tree tips for which there are no phylogenetic inputs 4 <code>boolean</code> false <p>1:      If a custom phylogenetic tree was provided by user (with <code>--phytree</code> parameter),     it's important to specify the type of tip labels.      Currently, two labeling schemes are supported:</p> <ul> <li>Latin binomials in <code>Genus_species</code> format (e.g., \"Homo_sapiens\").     NB! whitespaces in the name and sub-species ranks (variety, strain, form, etc.) are not allowed!</li> <li>Open Tree of Life IDs (e.g., \"ott770315\")</li> </ul> <p>2:     In the case if tree tips are in OTT-format,      it is possible to limit the query scope at Taxonomic Name Resolution Service of Open Tree of Life.      List available taxonomic contexts in <code>rotl</code> v.3.0.12 package:</p> <ul> <li>All life (default)</li> <li>Animals <ul> <li>Birds</li> <li>Tetrapods</li> <li>Mammals</li> <li>Amphibians</li> <li>Vertebrates</li> <li>Arthropods</li> <li>Molluscs</li> <li>Nematodes</li> <li>Platyhelminthes</li> <li>Annelids</li> <li>Cnidarians</li> <li>Arachnids</li> <li>Insects</li> </ul> </li> <li>Land plants <ul> <li>Hornworts</li> <li>Mosses</li> <li>Liverworts</li> <li>Vascular plants</li> <li>Club mosses</li> <li>Ferns</li> <li>Seed plants</li> <li>Flowering plants</li> <li>Monocots</li> <li>Eudicots</li> <li>Rosids</li> <li>Asterids</li> <li>Asterales</li> <li>Asteraceae</li> <li>Aster </li> <li>Symphyotrichum</li> <li>Campanulaceae</li> <li>Lobelia</li> </ul> </li> <li>Fungi <ul> <li>Basidiomycetes</li> <li>Ascomycetes</li> </ul> </li> <li>Bacteria</li> <li>Archaea</li> <li>SAR group<ul> <li>Excavata</li> <li>Amoebozoa</li> <li>Centrohelida</li> <li>Haptophyta</li> <li>Apusozoa</li> <li>Diatoms</li> <li>Ciliates</li> <li>Forams</li> </ul> </li> </ul> <p>Taxonomic context</p> <p>Whitespace is not allowed in taxonomic context!  Please replace space with underscore (e.g., \"Seed plants\" should be speciefied as <code>--taxgroup Seed_plants</code>).</p> <p>3:     In some cases, when there are no age estimates available for the phylogenetic tree nodes,      automatic tree retrieval could fail. Therefore, it could be helpful to override the maximum node age to obtain the tree.      Note, however, that in this case, units of some of the diversity indices (e.g., PD) become arbitrary.</p> <p>4:     Open Tree utilizes taxonomic data to augment the structure and completeness of the synthetic tree      when source phylogenies are absent or sparsely sampled (see Hinchliff et al., 2015).      If required, branches lacking phylogenetic support could be removed from the tree using <code>--phyloonly</code> argument.</p> <p>Open Tree of Life</p> <p>More information about Open Tree of Life is available at https://opentreeoflife.github.io/.</p>"},{"location":"parameters/#generic-options","title":"Generic options","text":"Parameter Description Type Default <code>--deriveddataset</code> Export list of GBIF dataset keys for the filtered species occurrences 1 <code>boolean</code> True <code>--help</code> Display help text (pipeline) <code>boolean</code> <code>--helpMsg</code> Display help text (pipeline) <code>boolean</code> <p>1: Could be used for citation and preparing a derived dataset with unique DOI.</p>"},{"location":"parameters/#nextflow-specific-parameters","title":"Nextflow-specific parameters","text":"<p>Parameter types</p> <p>PhyloNext-specific parameters are specified using double dash prefix (e.g., <code>--something value</code>).  Parameters related to the Nextflow workflow manager starts with a single dash prefix (e.g., <code>-work-dir wd</code>)</p> Parameter Description Type Example Default <code>-qs</code> Queue size (max number of processes that can be executed in parallel) <code>integer</code> 8 number of available CPUs <code>-w</code> Path to the working directory to store intermediate results <code>string</code> \"$(pwd)/wd\" \"work\" <code>-resume</code> Execute the pipeline using the cached results.Useful to continue executions that was stopped by an error <code>-profile</code> Configuration profiles (set of configuration attributes) 1 <code>string</code> test,docker <code>-params-file</code> Parameter file in YAML or JSON format 1 <code>file</code> <code>Mammals.yaml</code> <code>-c</code> Configuration file 1 <code>file</code> <code>nextflow.config</code> <p>1:     For examples, see the <code>Configuration and profiles</code> and <code>Parameter file</code> sections in the Usage documentation.</p> <p>Configuration profiles</p> <p>For more information on Nextflow profiles, see https://www.nextflow.io/docs/latest/config.html#config-profiles.</p>"},{"location":"post/","title":"PhyloNext results exploration and analysis","text":"<ul> <li>Manually configure static map using the output in GeoPackage format  </li> <li>Explore the results using Biodiverse program  </li> <li>Prepare citable derived dataset with unique DOI</li> </ul>"},{"location":"post/#constructing-maps-using-the-output-in-geopackage-format","title":"Constructing maps using the output in GeoPackage format","text":"<p>GeoPackage is an open format to store geospatial information.  It allows to store vector features (e.g., H3 polygons) along with their attributes (e.g., diversity estimates). As one of the outputs, PhyloNext provides <code>03.Plots/Diversity_estimates.gpkg</code> file,  which could be easily exported and visualized in your favorite GIS software.</p> <p>For example, you may use a free and open source application <code>QGIS</code> to open a GeoPackage file. </p>"},{"location":"post/#results-exploration-with-biodiverse-program","title":"Results exploration with Biodiverse program","text":"<p><code>Biodiverse</code> is a program for the spatial analysis of diversity which performs all the hard work for PhyloNext.  By default, PhyloNext output in Biodiverse format is stored in the <code>02.Biodiverse_results/Biodiverse.bds</code> file.  This file could be exported to Biodiverse and visualized using graphical user interface.</p> <p></p> <p>Biodiverse installation</p> <p>Please follow this guide to install Biodiverse.</p> <p>For more details on visualization of spatial analysis results on the phylogenetic tree, see these two blog posts ( 1 and  2 )  by Shawn Laffan.</p>"},{"location":"post/#derived-datasets","title":"Derived datasets","text":"<p>Derived datasets are citable records (with a unique DOI)  representing GBIF-mediated species occurrences that has been filtered significantly.  To create a derived dataset, PhyloNext provides a table with a list of the GBIF datasets (defined as <code>datasetKey</code>) from which the data originated.</p> <p>User can register new derived dataset at GBIF.org following this link: https://www.gbif.org/derived-dataset</p> <p>For additional information please see this GBIF blog post by Daniel Noesgaard.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions users might run into while using PhyloNext.</p> <p>Don't see your error/bug? Post an issue on GitHub</p> <p>If you've encountered an error or bug not seen here, please post an issue at PhyloNext GitHub Issues. This will help greatly to track down the error and fix it!</p>"},{"location":"troubleshooting/#pipeline-errors","title":"Pipeline errors","text":""},{"location":"troubleshooting/#resume-the-workflow","title":"Resume the workflow","text":"<p>Pipeline was interrupted</p> <p>Any accidental failure - e.g., you closed the terminal, rebooted your computer, or the task was killed by a scheduler on HPC.</p> <p>If the pipeline is interrupted, it can be resumed without having to start from scratch. Add the <code>-resume</code> flag to your command, and Nextflow will start the workflow from the last successfully executed process,  retrieving all previous results from the cache.</p>"},{"location":"troubleshooting/#the-number-of-available-cpus","title":"The number of available CPUs","text":"<p>Number of CPUs required</p> <p><code>Process requirement exceeds available CPUs -- req: 10; avail: 8</code></p> <p>Most likely, the number of CPUs configured for a process is larger than available on your system.  To fix it, modify a config file and reduce the number of CPUs for workflow processes to fit your hardware. E.g., change <pre><code>    withName:occ_filter{\ncpus = 10\n}\n</code></pre> to  <pre><code>    withName:occ_filter{\ncpus = 8\n}\n</code></pre></p> <p>See the <code>Configuration</code> section in the usage documentation.</p>"},{"location":"troubleshooting/#pipeline-revision-version","title":"Pipeline revision version","text":"<p>Pipeline revision version</p> <pre><code>Project \\`vmikk/phylonext\\` is currently stickied on revision: main -- you need to explicitly specify a revision with the option \\`-r\\` in order to use it\n</code></pre> <p>Solution - specify a version number of the pipeline. To run the latest version, add <code>-r main</code> to your command. Alternatively, you may specify the exact version (or tag) you wish, e.g. <code>-r v.0.0.2</code>.  </p>"},{"location":"troubleshooting/#installation-problems","title":"Installation problems","text":""},{"location":"troubleshooting/#nextflow-not-found","title":"Nextflow not found","text":"<p>Command not found</p> <p><code>command not found: nextflow</code></p> <p>If you installed Nextflow in the <code>~/bin</code> directory, most likely, it is not added to your PATH environment variable.  To fix it, run the following command:</p> <p><pre><code>export PATH=\"$HOME/bin:$PATH\"\n</code></pre> and try to execute Nextflow again.  </p> <p>If it works, you may add this command to the <code>~/.bashrc</code> file to make these changes permanent.  To do it from the command line, run:  </p> <pre><code>echo \"export PATH=$HOME/bin:$PATH\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Notes on shells other than bash</p> <p>If you are using a shell other than <code>bash</code>, please modify configuration file specific for your shell. E.g., for <code>zsh</code> it should be <code>~/.zshrc</code>. To know which shell are you using, run <code>echo $SHELL</code></p>"},{"location":"troubleshooting/#github-api-rate-limits","title":"GitHub API rate limits","text":"<p>GitHub API rate limits</p> <pre><code>Checking vmikk/PhyloNext ...  \nWARN: Cannot read project manifest -- Cause: API rate limit exceeded -- Provide your GitHub user name and password to get a higher rate limit  \nAPI rate limit exceeded -- Provide your GitHub user name and password to get a higher rate limit\n</code></pre> <p>Without authentication, GitHub allows only a limited number of connections per hour (60 unauthenticated requests/hr).  Most likely, you've hit this problem because users from your organization (e.g., university) send too many requests to GitHub.  </p> <p>To overcome this limitation, you may generate a personal access token at https://github.com/settings/tokens (the minimum required scopes are <code>repo</code>, <code>read:org</code>, and <code>workflow</code>). To authenticate, you may use the GitHub client <code>gh</code> (https://cli.github.com/). Run <code>gh auth login --with-token &lt;YOURTOKEN&gt;</code> to login. This way you will have a higher API rate limit.  </p> <p>Alternatively, you may wait for a while until the recovery of API limits.  Or, if you are using PhyloNext locally, you may change your IP address  (for example, connect your laptop to another wi-fi hotspot, e.g., to a smartphone hotspot)  and try to pull the pipeline once again.  You need to download the pipeline from GitHub only once.  Then, Nextflow will cache the pipeline code locally in the <code>~/.nextflow/assets/</code> directory  and will re-use it later without the need to connect to GitHub.  </p> <p>As another option, you may just manually clone the git repository into Nextlow assets directory: <pre><code>mkdir -p ~/.nextflow/assets/vmikk/\ncd ~/.nextflow/assets/vmikk/\ngit clone https://github.com/vmikk/PhyloNext\n</code></pre></p> <p>Rate limit information</p> <p>To find the current rate limit information (for unauthenticated requests), run <code>curl -I https://api.github.com/users/octocat</code></p> <p>Token security</p> <p>It is important to keep your token secure and not share it with others. Also make sure to revoke the token if it is compromised or no longer needed.</p>"},{"location":"troubleshooting/#docker-permissions","title":"Docker permissions","text":"<p>Docker - permission denied</p> <p><code>docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post \"http://%2Fvar%2Frun%2Fdocker.sock/v1.24/containers/create?name=nxf-Cz6yqNjGI9AhRpsqkU4mSAHC\": dial unix /var/run/docker.sock: connect: permission denied.</code></p> <p>Most likely, you forgot to add your user to the <code>docker</code> group and you need a super-user privileges to execute the container. To fix this error, run:  </p> <pre><code># Create the docker group.\nsudo groupadd docker\n\n#  Add your user to the docker group.\nsudo usermod -aG docker ${USER}\n# Log out and log back in so that your group membership is re-evaluated.\n# Alternatively, to skip logging out, type the following command:  su -s ${USER}\n# Verify that you can run docker commands without sudo\ndocker run hello-world\n</code></pre>"},{"location":"troubleshooting/#docker-and-wsl2","title":"Docker and WSL2","text":"<p>Docker and WSL2</p> <p><code>The command 'docker' could not be found in this WSL 2 distro. We recommend to activate the WSL integration in Docker Desktop settings.</code></p> <p>Try to enable <code>Use the WSL2 based engine</code> in the <code>Docker Desktop</code> settings.  </p> <p> </p>"},{"location":"troubleshooting/#phylonext-containers","title":"PhyloNext containers","text":"<p>If for some reason Nextflow is not able to download containers for PhyloNext,  it's possible to pull them manually using:</p> <pre><code>docker pull vmikk/rarrow:1.0.0\ndocker pull vmikk/biodiverse:1.0.0\ndocker pull vmikk/opentree:0.0.2\n</code></pre>"},{"location":"usage/","title":"Usage instructions","text":""},{"location":"usage/#input-data","title":"Input data","text":"<p>Input data</p> <p>For mode details, see the input data description section in the documentation.</p>"},{"location":"usage/#run-test-example","title":"Run test example","text":"<p>Test run the pipeline using test data and Docker engine: <pre><code>nextflow run vmikk/phylonext -r main -profile test,docker\n</code></pre></p>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:  </p> <p><pre><code>nextflow run vmikk/phylonext -r main \\\n--input \"/mnt/GBIF/Parquet/2022-01-01/occurrence.parquet/\" \\\n--classis \"Mammalia\" --family  \"Felidae,Canidae\" \\\n--country \"DE,PL,CZ\"  \\\n--minyear 2000  \\\n--dbscan true  \\\n--iterations 100  \\\n--outdir \"$PWD\" \\\n-resume \\\n-profile docker\n</code></pre> This will launch the pipeline with the <code>docker</code> configuration profile (see below for more information about profiles).</p> <p>Parameters</p> <p>All parameters are listed in the <code>Parameters</code> section of the documentation.  </p> <p>Note that the pipeline will create the following files in your working directory:  </p> Directory or file Description <code>00.filtered1.parquet</code> Parquet subset of the data <code>01.filtered2</code> Data with spatial outliers removed, individually for each species in the dataset <code>01.NumRecords</code> Counts of the number of records per grid cell <code>02.Biodiverse_input</code> Table with species occurrences and phylogenetic tree, as well as input files for Biodiverse <code>02.Biodiverse_results</code> Biodiverse output <code>03.Plots</code> Data visualizations <code>work</code> Directory containing the Nextflow working files <code>results</code> Pipeline execution reports and dataset DOIs <code>.nextflow_log</code> Log file from Nextflow <p>For more details about output files, see the Outputs section of documentation.</p>"},{"location":"usage/#workflow-caching-and-checkpointing","title":"Workflow caching and checkpointing","text":"<p>All intermediate results of the pipeline run are stored in the <code>work</code> and <code>.nextflow</code> directories.  To restart a pipeline following an error from the most recent successfully executed process, use the '-resume' flag.  This way, Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it strop previously.  This option could also be helpful if you wish to change some of the parameters of downstream processes and will allow reusing results of upstream steps.  </p> <p>Without <code>-resume</code>, Nextflow will start the workflow from scratch and overwrite all previous results.  </p>"},{"location":"usage/#built-in-data","title":"Built-in data","text":"<p>PhyloNext pipeline contains several files for removing spatial outliers or data subsetting. By default, built-in data is stored in the folder <code>${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/</code>.</p> File Description Citation <code>Land_Buffered_025_dgr.RData</code> Land polygon for removal of non-terrestrial occurrences Natural Earth <code>CC_CountryCentroids_buf_1000m.RData</code> Polygons with country and province centroids Zizka et al., 2019 <code>CC_Capitals_buf_10000m.RData</code> Polygons with country capitals Zizka et al., 2019 <code>CC_Institutions_buf_100m.RData</code> Polygons with biological institutions and museums Zizka et al., 2019 <code>CC_Urban.RData</code> Polygons with urban areas Natural Earth <code>WGSRPD.RData</code> Polygons of World Geographical Regions WGSRPD <code>WorldMap_NaturalEarth_Medium.RData</code> World map Natural Earth <p>To use the built-in data, provide a full path to the data. E.g., to remove records from urban areas, add the following parameter to your command: <code>--rmurban ${HOME}/.nextflow/assets/vmikk/phylonext/pipeline_data/CC_Urban.RData</code>. Alternatively, you may use custom files prepared in the same format.</p> <p>To remove common spatial errors from the data, PhyloNext uses databases  from the <code>CoordinateCleaner</code> R package by Alexander Zizka.  The following buffers were added to the datasets because coordinates in a database could be slightly inaccurate due to errors: </p> <ul> <li>Country and province centroids - 1 km buffer  </li> <li>Capital coordinates - 10 km buffer  </li> <li>Coordinates of biodiversity institutions - 100 m buffer  </li> </ul> <p>For the removal of non-terrestrial samples, PhyloNext uses a shapefile from Natural Earth, with a 0.25-degree buffer (corresponding to ~27 km distance, which is approximately half of the H3 grid cell at resolution 4).  </p> <p>Please cite any datasets if you have used them in your analysis.  </p>"},{"location":"usage/#workflow-processes","title":"Workflow processes","text":"<p>Workflow managers such as Nextflow are designed to help users run complex pipelines of processes,  potentially involving multiple steps and dependencies between them, efficiently and reproducibly.  One of the key features of such tools is the ability to run processes in parallel,  which can reduce the overall time taken to complete a workflow by distributing the workload across multiple computational resources.  To run the pipeline effectively and maximize resource utilization, Nextflow will:  </p> <ul> <li>Effectively and automatically allocate CPUs on the available computational resources (local, a cluster, or a cloud environment);</li> <li>Use a scheduling algorithm to determine the order in which processes should be run based on the dependencies between them and the availability of resources.</li> </ul> <p>During the pipeline execution, you will see a progress indicator in the output in your terminal:</p> <pre><code>[ad/02689a] process &gt; occ_filter            [100%] 1 of 1 \u2714\n[b7/48eb93] process &gt; record_count          [100%] 1 of 1 \u2714\n[cc/af139c] process &gt; outl_low              [100%] 1 of 1 \u2714\n[88/197e96] process &gt; outl_high (5219243)   [100%] 6 of 6 \u2714\n[85/8425c9] process &gt; merge_occ             [100%] 1 of 1 \u2714\n[b6/bcf57e] process &gt; prep_biodiv           [100%] 1 of 1 \u2714\n[aa/bbb119] process &gt; phylodiv (1)          [100%] 1 of 1 \u2714\n[18/d367ac] process &gt; rand_filelist         [100%] 1 of 1 \u2714\n[6b/b1d08b] process &gt; aggregate_rnds_biodiv [100%] 1 of 1 \u2714\n[c6/ecf435] process &gt; div_to_csv            [100%] 1 of 1 \u2714\n[ae/7de318] process &gt; plot_pd               [100%] 1 of 1 \u2714\n[ba/141853] process &gt; plot_leaflet          [100%] 1 of 1 \u2714\n[6a/ca28a8] process &gt; derived_datasets      [100%] 1 of 1 \u2714\n</code></pre> <p>Processes are run in parallel and start as soon as their input data becomes available.  Pipeline scripts are executed by Nextflow in a separate working directory.  Each working directory contains input files (symbolic links in case of running the pipeline locally), as well as results and log files.</p> <p>Log files</p> <p>In case of pipeline failure, it could be useful to inspect logs for a failed task. E.g., working directory for the <code>occ_filter</code> is located in <code>./work/ad/02689a...</code>  and contains a log file <code>.command.log</code>.  </p> <p>Processes description:  </p> Process name Description CPUs per task 1 <code>occ_filter</code> Occurrence filtering of parquet file 4 <code>record_count</code> Counting the total number of records per H3 cell 4 <code>outl_high</code> Per-species outlier removal for species with high occurrence (optionally, with DBSCAN), spatial binning 1 <code>outl_low</code> Spatial outlier removal for species with high occurrence (DBSCAN disabled), spatial binning 4 <code>prep_ott_ids</code> Preparation of Open Tree IDs for a phylogenetic tree (optional) 4 <code>get_ott_tree</code> Automatic fetching of a phylogenetic tree from Open Tree of Life (optional) 1 <code>merge_occ</code> Merging filtered species occurrences, species name matching, phylogenetic tree pruning 4 <code>prep_biodiv</code> Creation of Biodiverse input files and estimation of the observed diversity 1 <code>phylodiv</code> Randomization of species communities and estimation of standardized effect sizes of biodiversity metrics 1 <code>rand_filelist</code> Preparation of randomization results 1 <code>aggregate_rnds_biodiv</code> Randomization results pooling 1 <code>div_to_csv</code> Export of Biodiverse results into CSV format 1 <code>plot_pd</code> Plotting diversity indices (non-interactive maps in PDF format) 1 <code>plot_leaflet</code> Plotting diversity indices (interactive maps - Leaflet-based choropleth) 1 <code>derived_datasets</code> Preparation of a list of DOIs for the GBIF datasets involved in the analysis (optional) 4 <p>1:     The default number of CPUs per task is configured in the <code>nextflow.config</code>.</p> <p>Processes are executed by Nextflow independently and are isolated from each other,  which allows running the pipeline on different nodes at HPC or a cloud platform. Most workflow processes support multi-threading (e.g., species occurrence filtering).  In some cases, the task will be divided into chunks and run in parallel  (e.g., estimation of standardized effect sizes - 10 processes with 100 randomization iterations each).  It is possible to customize the number of CPUs required by the process task (see <code>Configuration</code> below).  The total number of CPUs allowed to use by PhyloNext can be adjusted with <code>-qs</code> parameter (by default, all available CPUs will be used).</p>"},{"location":"usage/#getting-help","title":"Getting help","text":"<p>To show a help message, run: <pre><code>nextflow run vmikk/phylonext -r main --help\n</code></pre></p>"},{"location":"usage/#configuration-and-profiles","title":"Configuration and profiles","text":"<p>Nextflow has a very flexible pipeline configuration system that supports multiple options  such as parameter files, configuration files, and profiles.  </p>"},{"location":"usage/#parameter-file","title":"Parameter file","text":"<p>As the number of parameters can be quite large, it is possible to pass the pipeline parameters via YAML or JSON file, e.g.: <pre><code>nextflow run vmikk/phylonext -r main -resume -params-file Mammals.yaml\n</code></pre></p> <p>The YAML file could contain the following: <pre><code>input      : \"/mnt/GBIF/Parquet/2022-01-01/occurrence.parquet/\"\nclassis    : \"Mammalia\"\nfamily     : \"Felidae,Canidae\"\ncountry    : \"DE,PL,CZ\"\nminyear    : 2000\ndbscan     : true\nphytree    : \"/path/to/the/phylogenetic/tree.nwk\"\niterations : 100\n</code></pre></p>"},{"location":"usage/#configuration-file","title":"Configuration file","text":"<p>While the default requirements set within the pipeline will hopefully work for most people and with most input data,  you may want to customize the compute resources that the pipeline requests.  Each step in the pipeline has a default set of requirements for the number of CPUs  (and optionally, RAM and execution time - which could be important for executing PhyloNext on the HPC cluster).  </p> <p>To adjust the resource usage (e.g., the number of CPUs required by the <code>occ_filter</code> process),  you may create a custom configuration file (e.g., <code>my.config</code>) of the form:  </p> <pre><code>process {\n// Occurrence filtering, stage I\nwithName:occ_filter{\ncpus = 10\n}\n}\n</code></pre> <p>Then, use the <code>-c</code> option to choose the created configuration file while running the pipeline. E.g., <pre><code>nextflow run vmikk/phylonext ... -c my.config\n</code></pre></p> <p>Configs</p> <p>For more information, see the Nextflow documentation - Configuration.</p> <p>Multiple configs</p> <p>When a pipeline is launched, Nextflow looks for configuration files (and profiles) in multiple locations.  The configs will be merged, and the settings in the config with higher priority will override the settings in the other configs.  If you want to ignore any default configuration files and use only the custom one, use the <code>-C your.config</code> option (please note that the <code>C</code> letter is capitalized).  For more details, see the Configutation section in the Nextflow docs.</p>"},{"location":"usage/#profiles","title":"Profiles","text":"<p>Nextflow profiles can be used as configuration presets for different compute environments.  </p> <p>We highly recommend using Docker or Singularity containers for full pipeline reproducibility.  For this, several generic profiles are bundled with the pipeline, instructing PhyloNext to use software that is packaged in the containers. Available profiles include:</p> Profile name Description <code>docker</code> A generic configuration profile to be used with Docker <code>singularity</code> A generic configuration profile to be used with Singularity <code>test</code> 1 A profile with a complete configuration for automated testing (built-in phylogenetic tree) <code>test_ott</code> 1 A profile with a complete configuration for automated testing (phylogenetic tree will be fetched from OToL) <p>1:     Test profiles include links to test data and do not require any other parameters to be specified.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>.  This is not recommended.  </p> <p>It is worth noting that multiple profiles can be loaded, for example: <code>-profile test,docker</code>.  Please keep in mind that the order of arguments is important!  Profiles are loaded in sequential order so that the later profile will overwrite earlier profiles.  </p> <p>Cluster and cloud executors</p> <p>To create a profile for running PhyloNext on HPC (e.g., using SLURM) or cloud (e.g., Azure Batch or AWS Batch),  please address Nextflow documentation - Executor section</p>"},{"location":"usage/#the-other-helpful-commands","title":"The other helpful commands","text":"<p>To make sure that you're running the latest version of the pipeline,  you may download the latest version and update the pipeline using: <pre><code>nextflow pull vmikk/phylonext\n</code></pre> By default, the cached version of the pipeline is stored in the <code>~/.nextflow/assets/vmikk/PhyloNext</code> directory.</p> <p>Run the latest development version of the pipeline (from the <code>main</code> branch of the code repository): <pre><code>nextflow run vmikk/phylonext -r main ...\n</code></pre></p> <p>When executing PhyloNext on your data, it is a good idea to specify a version number for the pipeline.  This ensures that specific code revision and software dependencies are used.  Even if the codebase has changed, if you continue using the same tag, you will run the same pipeline version. To find the latest version number, go to the PhyloNext releases page.  Then, when launching the pipeline, specify the version using <code>-r</code> flag (e.g., <code>-r v.1.0.0</code>). This version number will be recorded in pipeline reports so you can later go back and know what version you used.  </p> <p>Run the tagged version (e.g., v.1.3.0) of the pipeline: <pre><code>nextflow run vmikk/phylonext -r v.1.3.0 ...\n</code></pre></p> <p>Print the pipeline and system runtime information: <pre><code>nextflow info\nnextflow info vmikk/phylonext\n</code></pre></p> <p>Delete the local copy of the pipeline: <pre><code>nextflow drop vmikk/phylonext\n</code></pre></p>"},{"location":"usage/#docker-and-singularity-containers","title":"Docker and Singularity containers","text":"<p>Ready-to-use containers with all software dependencies required for PhyloNext  are hosted on Docker Hub and  Singularity Library.  </p> <p>If you need to use a different version of a tool with the pipeline,  you may re-build the default containers and override the location of a container in a custom configuration file.  PhyloNext currently relies on three containers and refers to them using process labels.  </p> Process Label Default Docker container Docker file Default Singularity container Singularity definition file <code>container_r</code> <code>vmikk/rarrow:1.3.0</code> Link <code>library://vmiks/gbif/rarrow:1-3-0</code> Link <code>container_biodiverse</code> <code>vmikk/biodiverse:1.3.0</code> Link <code>library://vmiks/gbif/biodiverse:1-3-0</code> Link <code>container_ott</code> <code>vmikk/opentree:0.0.2</code> Link <code>library://vmiks/gbif/opentree:0-0-2</code> Link <p>The configuration file has the following format and must be adjusted if you would like to use custom containers:  </p> <p><pre><code>process {\nwithLabel: 'container_r' {\ncontainer = 'vmikk/rarrow:1.3.0'\n}\nwithLabel: 'container_biodiverse' {\ncontainer = 'vmikk/biodiverse:1.3.0'\n}\nwithLabel: 'container_ott' {\ncontainer = 'vmikk/opentree:0.0.2'\n}\n}\n</code></pre> Containers can either be pulled from the web (e.g., use <code>youraccount/containername</code> to get it from Docker Hub),  or you may use local files (specify tag name for Docker, or a full path to the Singularity file).  </p>"},{"location":"usage/#docker-image","title":"Docker image","text":"<p>To build the Docker image with Biodiverse (<code>container_biodiverse</code>), run: <pre><code>git clone https://github.com/vmikk/biodiverse-docker\ncd biodiverse-docker\ndocker build --tag biodiverse --file Dockerfile_NoPerlbrew . </code></pre></p>"},{"location":"usage/#singularity-image","title":"Singularity image","text":"<p>To build the Singularity image with Biodiverse (<code>container_biodiverse</code>), run: <pre><code>git clone https://github.com/vmikk/biodiverse-docker\ncd biodiverse-docker\nsudo singularity build Biodiverse.sif SingularityDef_NoPerlbrew.def\n</code></pre></p>"},{"location":"usage/#system-requirements","title":"System requirements","text":"<p>When running the pipeline, it's important to consider the system requirements and time elapsed to complete the analysis.  </p> <p>For example, for the global analysis of Mammal diversity at H3 resolution = 3  (4710 species and 9814 grid cells in total), and using 5000 spatially-uncontrained randomizations, with DBSCAN removal of outliers,  the total CPU hours required was 113.6 hours (using 50 CPUs, the wall time was 4 hours).  The peak memory usage during the analysis was 62GB for a DBSCAN filtering of occurrences of a very abundant species.  This analysis is computationally intensive and requires a powerful computer with at least 64GB of RAM.</p> <p>In contrast, the same analysis with a basis of record restricted to <code>PRESERVED_SPECIMEN</code>  (4333 species and 7182 grid cells)  required 30.4 CPU hours and a wall time of 1 hour and 13 minutes.  The maximum memory usage was 9GB for an interactive map generation.</p> <p>For a smaller-scale analysis, usually a less powerful computer with at least 16GB of RAM should be sufficient.</p>"},{"location":"usage/#running-on-hpc","title":"Running on HPC","text":"<p>If you would like to run PhyloNext on HPC cluster, Nextflow can handle job submissions and supervise the running jobs.  In this case the Nextflow process must run until the pipeline is finished.  For this purpose you may either use <code>tmux</code> or <code>screen</code> to create a detached terminal session which you can log back into at a later time.  Or you may submit Nextflow process using your job scheduler (e.g., SLURM), from where Nextlow will start submitting the other jobs.  You can also use the <code>-bg</code> flag to launch Nextflow in the background to avoid blocking your current session.  </p> <p>Depending on the number of tasks, in some cases, the Nextflow Java virtual machines can require a large amount of memory.  To specify the RAM limits, you may create the following environmental variable before running Nextflow:  </p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"},{"location":"usage/#whats-next","title":"What's next?","text":"<ul> <li>Explore interactive maps (see Output file description)  </li> <li>Manually configure static map using the output in GeoPackage format (see Postprocessing)  </li> <li>Explore the results using Biodiverse program (see Postprocessing)  </li> <li>Prepare citable derived dataset with unique DOI (see Postprocessing)  </li> </ul>"},{"location":"usecases/","title":"Use cases","text":"<p>Here, we will showcase several maps generated using the PhyloNext pipeline for different use cases.  </p>"},{"location":"usecases/#phylogenetic-diversity-of-terrestrial-mammals-based-on-the-iucn-datasets","title":"Phylogenetic diversity of terrestrial Mammals based on the IUCN datasets","text":"<p>Phylogenetic diversity </p> <p>Centers of endemism based on the CANAPE analysis </p> <p>Data sources:</p> <ul> <li> <p>The IUCN Red List of Threatened Species https://www.iucnredlist.org/resources/spatial-data-download</p> </li> <li> <p>Phylogenetic tree Gumbs R, Gray CL, B\u00f6hm M, Burfield IJ, Couchman OR, Faith DP, Forest F, Hoffmann M, Isaac NJB, Jetz W, Mace GM, Mooers AO, Safi K, Scott O, Steel M, Tucker CM, Pearse WD, Owen NR, Rosindell J (2023) The EDGE2 protocol: Advancing the prioritisation of Evolutionarily Distinct and Globally Endangered species for practical conservation action. PLOS Biology, 21(2), e3001991. DOI:10.1371/journal.pbio.3001991 Upham NS, Esselstyn JA, Jetz W (2019) Inferring the mammal tree: Species-level sets of phylogenies for questions in ecology, evolution, and conservation. PLoS Biology, 17(12), e3000494. DOI:10.1371/journal.pbio.3000494 </p> </li> </ul>"},{"location":"webgui/","title":"Web GUI","text":"<p>To facilitate easy and efficient navigation for exploring the PhyloNext pipeline,  a user-friendly, web-based graphical user interface (GUI) has been developed  by Thomas Stjernegaard Jeppesen.  </p> <p>The GUI is available at https://phylonext.gbif.org/.  </p> <p>GBIF user account required</p> <p>To access the GUI, users must have a GBIF user account. To register an account, please visit https://www.gbif.org/.</p> <p>This interface offers a simplified approach to accessing phylogenetic diversity information,  requiring minimal technical expertise. Built as a single-page application  using the React.js JavaScript framework, the GUI effectively controls  the PhyloNext pipeline by communicating with a REST web service written in Node.js.  </p> <p>WebGUI - configuration screen </p> <p>WebGUI - run progress </p> <p>WebGUI - results screen </p> <p>The source code for the GUI can be found at  https://github.com/gbif/phylonext-ui/,  while the web service code is available at  https://github.com/gbif/phylonext-ws.  </p> <p>Issues with the GUI</p> <p>If you encounter any errors, please kindly report them on the GitHub issue tracker:  https://github.com/gbif/phylonext-ui/issues </p>"}]}